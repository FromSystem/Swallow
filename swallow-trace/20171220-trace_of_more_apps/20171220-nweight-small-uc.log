17/12/20 16:24:29 INFO SparkContext: Running Spark version 2.2.0
17/12/20 16:24:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/12/20 16:24:30 INFO SparkContext: Submitted application: NWeightGraphX
17/12/20 16:24:30 INFO SecurityManager: Changing view acls to: hadoop
17/12/20 16:24:30 INFO SecurityManager: Changing modify acls to: hadoop
17/12/20 16:24:30 INFO SecurityManager: Changing view acls groups to:
17/12/20 16:24:30 INFO SecurityManager: Changing modify acls groups to:
17/12/20 16:24:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
17/12/20 16:24:30 INFO Utils: Successfully started service 'sparkDriver' on port 43031.
compressShuffle => [spark.shuffle.compress : false]
17/12/20 16:24:30 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:30 INFO SparkEnv: Registering MapOutputTracker
17/12/20 16:24:30 INFO SparkEnv: Registering BlockManagerMaster
17/12/20 16:24:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/12/20 16:24:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/12/20 16:24:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12379fe6-db01-409c-8c7e-40f830b23197
17/12/20 16:24:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
17/12/20 16:24:30 INFO SparkEnv: Registering OutputCommitCoordinator
17/12/20 16:24:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/12/20 16:24:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.10.43.130:4040
17/12/20 16:24:31 INFO SparkContext: Added JAR file:/home/hadoop/hadoopTest/HiBench/sparkbench/assembly/target/sparkbench-assembly-6.1-SNAPSHOT-dist.jar at spark://10.10.43.130:43031/jars/sparkbench-assembly-6.1-SNAPSHOT-dist.jar with timestamp 1513758271071
17/12/20 16:24:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://cluster209-hadoop-master:7077...
17/12/20 16:24:31 INFO TransportClientFactory: Successfully created connection to cluster209-hadoop-master/10.10.43.130:7077 after 23 ms (0 ms spent in bootstraps)
17/12/20 16:24:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20171220162431-0007
17/12/20 16:24:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171220162431-0007/0 on worker-20171220145248-10.10.43.132-42694 (10.10.43.132:42694) with 4 cores
17/12/20 16:24:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20171220162431-0007/0 on hostPort 10.10.43.132:42694 with 4 cores, 1024.0 MB RAM
17/12/20 16:24:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171220162431-0007/1 on worker-20171220145325-10.10.43.131-46830 (10.10.43.131:46830) with 4 cores
17/12/20 16:24:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20171220162431-0007/1 on hostPort 10.10.43.131:46830 with 4 cores, 1024.0 MB RAM
17/12/20 16:24:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44564.
17/12/20 16:24:31 INFO NettyBlockTransferService: Server created on 10.10.43.130:44564
17/12/20 16:24:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/12/20 16:24:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.10.43.130, 44564, None)
17/12/20 16:24:31 INFO BlockManagerMasterEndpoint: Registering block manager 10.10.43.130:44564 with 366.3 MB RAM, BlockManagerId(driver, 10.10.43.130, 44564, None)
17/12/20 16:24:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.10.43.130, 44564, None)
17/12/20 16:24:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.10.43.130, 44564, None)
17/12/20 16:24:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171220162431-0007/0 is now RUNNING
17/12/20 16:24:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171220162431-0007/1 is now RUNNING
17/12/20 16:24:33 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/12/20 16:24:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.43.131:52928) with ID 1
17/12/20 16:24:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.43.132:33294) with ID 0
17/12/20 16:24:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.10.43.131:37275 with 366.3 MB RAM, BlockManagerId(1, 10.10.43.131, 37275, None)
17/12/20 16:24:36 INFO BlockManagerMasterEndpoint: Registering block manager 10.10.43.132:43277 with 366.3 MB RAM, BlockManagerId(0, 10.10.43.132, 43277, None)
17/12/20 16:24:36 INFO SparkContext:
******************** [KMDebug] ********************
Init with textFile
17/12/20 16:24:36 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 219.1 KB, free 366.1 MB)
17/12/20 16:24:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 366.1 MB)
17/12/20 16:24:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.10.43.130:44564 (size: 20.6 KB, free: 366.3 MB)
17/12/20 16:24:37 INFO SparkContext: Created broadcast 0 from textFile at GraphxNWeight.scala:74
17/12/20 16:24:37 INFO SparkContext: Starting job: saveAsTextFile at GraphxNWeight.scala:112
17/12/20 16:24:37 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:38 INFO FileInputFormat: Total input paths to process : 8
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 2 (flatMap at GraphxNWeight.scala:74)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 5 (map at GraphxNWeight.scala:85)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 12 (mapPartitions at VertexRDD.scala:356)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 7 (map at GraphxNWeight.scala:87)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 20 (mapPartitions at VertexRDDImpl.scala:247)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 24 (mapPartitions at GraphImpl.scala:207)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 32 (mapPartitions at VertexRDDImpl.scala:247)
17/12/20 16:24:38 INFO DAGScheduler: Registering RDD 36 (mapPartitions at GraphImpl.scala:207)
17/12/20 16:24:38 INFO DAGScheduler: Got job 0 (saveAsTextFile at GraphxNWeight.scala:112) with 8 output partitions
17/12/20 16:24:38 INFO DAGScheduler: Final stage: ResultStage 8 (saveAsTextFile at GraphxNWeight.scala:112)
17/12/20 16:24:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5, ShuffleMapStage 2, ShuffleMapStage 3, ShuffleMapStage 7)
17/12/20 16:24:38 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5, ShuffleMapStage 2, ShuffleMapStage 3, ShuffleMapStage 7)
17/12/20 16:24:38 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at flatMap at GraphxNWeight.scala:74), which has no missing parents
17/12/20 16:24:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 366.1 MB)
17/12/20 16:24:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 366.1 MB)
17/12/20 16:24:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.10.43.130:44564 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006
17/12/20 16:24:38 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at flatMap at GraphxNWeight.scala:74) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:24:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 8 tasks
17/12/20 16:24:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.10.43.131, executor 1, partition 0, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 10.10.43.132, executor 0, partition 1, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, 10.10.43.131, executor 1, partition 2, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, 10.10.43.132, executor 0, partition 3, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, 10.10.43.131, executor 1, partition 4, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, 10.10.43.132, executor 0, partition 5, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, 10.10.43.131, executor 1, partition 6, ANY, 4901 bytes)
17/12/20 16:24:38 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, 10.10.43.132, executor 0, partition 7, ANY, 4901 bytes)
17/12/20 16:24:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.10.43.132:43277 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.10.43.132:43277 (size: 20.6 KB, free: 366.3 MB)
17/12/20 16:24:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.10.43.131:37275 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.10.43.131:37275 (size: 20.6 KB, free: 366.3 MB)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 8487 ms on 10.10.43.132 (executor 0) (1/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 8490 ms on 10.10.43.132 (executor 0) (2/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 8561 ms on 10.10.43.132 (executor 0) (3/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 8651 ms on 10.10.43.131 (executor 1) (4/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 8664 ms on 10.10.43.132 (executor 0) (5/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 8719 ms on 10.10.43.131 (executor 1) (6/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8745 ms on 10.10.43.131 (executor 1) (7/8)
17/12/20 16:24:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 8751 ms on 10.10.43.131 (executor 1) (8/8)
17/12/20 16:24:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
17/12/20 16:24:46 INFO DAGScheduler: ShuffleMapStage 0 (flatMap at GraphxNWeight.scala:74) finished in 8.803 s
17/12/20 16:24:46 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:24:46 INFO DAGScheduler: running: Set()
17/12/20 16:24:46 INFO DAGScheduler: waiting: Set(ShuffleMapStage 1, ShuffleMapStage 5, ShuffleMapStage 2, ShuffleMapStage 6, ShuffleMapStage 3, ShuffleMapStage 7, ShuffleMapStage 4, ResultStage 8)
17/12/20 16:24:46 INFO DAGScheduler: failed: Set()
17/12/20 16:24:46 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at GraphxNWeight.scala:85), which has no missing parents
17/12/20 16:24:46 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:46 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.4 KB, free 366.1 MB)
17/12/20 16:24:46 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 366.1 MB)
17/12/20 16:24:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.10.43.130:44564 (size: 2.4 KB, free: 366.3 MB)
17/12/20 16:24:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/12/20 16:24:46 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at map at GraphxNWeight.scala:85) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:24:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks
17/12/20 16:24:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 8, 10.10.43.131, executor 1, partition 0, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 9, 10.10.43.132, executor 0, partition 1, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 10, 10.10.43.131, executor 1, partition 2, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO DAGScheduler: Submitting ShuffleMapStage 2 (VertexRDD.createRoutingTables - vid2pid (aggregation) MapPartitionsRDD[12] at mapPartitions at VertexRDD.scala:356), which has no missing parents
17/12/20 16:24:46 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 11, 10.10.43.132, executor 0, partition 3, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 12, 10.10.43.131, executor 1, partition 4, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 13, 10.10.43.132, executor 0, partition 5, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 14, 10.10.43.131, executor 1, partition 6, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:46 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 15, 10.10.43.132, executor 0, partition 7, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 4.1 KB, free 366.0 MB)
17/12/20 16:24:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.3 KB, free 366.0 MB)
17/12/20 16:24:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.10.43.130:44564 (size: 2.3 KB, free: 366.3 MB)
17/12/20 16:24:46 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1006
17/12/20 16:24:46 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (VertexRDD.createRoutingTables - vid2pid (aggregation) MapPartitionsRDD[12] at mapPartitions at VertexRDD.scala:356) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:24:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks
17/12/20 16:24:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.10.43.131:37275 (size: 2.4 KB, free: 366.3 MB)
17/12/20 16:24:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52928
17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 15934088
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 15934088
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 15934088
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 15934088
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 15934088
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 15934088
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 15934088
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 15934088
totalSize: 15934088


17/12/20 16:24:47 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 195 bytes
17/12/20 16:24:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.10.43.132:43277 (size: 2.4 KB, free: 366.3 MB)
17/12/20 16:24:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33294
17/12/20 16:24:47 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@79d64cae


17/12/20 16:24:48 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 16, 10.10.43.132, executor 0, partition 1, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:48 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 9) in 1639 ms on 10.10.43.132 (executor 0) (1/8)
17/12/20 16:24:48 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 17, 10.10.43.131, executor 1, partition 0, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:48 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 12) in 1827 ms on 10.10.43.131 (executor 1) (2/8)
17/12/20 16:24:48 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.10.43.132:43277 (size: 2.3 KB, free: 366.3 MB)
17/12/20 16:24:48 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 18, 10.10.43.132, executor 0, partition 3, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:48 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 15) in 1900 ms on 10.10.43.132 (executor 0) (3/8)
17/12/20 16:24:48 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 19, 10.10.43.132, executor 0, partition 5, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:48 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 11) in 1918 ms on 10.10.43.132 (executor 0) (4/8)
17/12/20 16:24:48 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 20, 10.10.43.132, executor 0, partition 7, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:48 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 13) in 1923 ms on 10.10.43.132 (executor 0) (5/8)
17/12/20 16:24:49 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.10.43.131:37275 (size: 2.3 KB, free: 366.3 MB)
17/12/20 16:24:49 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 21, 10.10.43.131, executor 1, partition 2, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:49 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 10) in 2106 ms on 10.10.43.131 (executor 1) (6/8)
17/12/20 16:24:49 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 22, 10.10.43.131, executor 1, partition 4, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:49 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 14) in 2129 ms on 10.10.43.131 (executor 1) (7/8)
17/12/20 16:24:49 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 23, 10.10.43.131, executor 1, partition 6, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 8) in 2151 ms on 10.10.43.131 (executor 1) (8/8)
17/12/20 16:24:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
17/12/20 16:24:49 INFO DAGScheduler: ShuffleMapStage 1 (map at GraphxNWeight.scala:85) finished in 2.152 s
17/12/20 16:24:49 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:24:49 INFO DAGScheduler: running: Set(ShuffleMapStage 2)
17/12/20 16:24:49 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 3, ShuffleMapStage 7, ShuffleMapStage 4, ResultStage 8)
17/12/20 16:24:49 INFO DAGScheduler: failed: Set()
17/12/20 16:24:49 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at map at GraphxNWeight.scala:87), which has no missing parents
17/12/20 16:24:49 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:49 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.0 KB, free 366.0 MB)
17/12/20 16:24:49 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.6 KB, free 366.0 MB)
17/12/20 16:24:49 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.10.43.130:44564 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:49 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/12/20 16:24:49 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at map at GraphxNWeight.scala:87) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:24:49 INFO TaskSchedulerImpl: Adding task set 3.0 with 8 tasks
17/12/20 16:24:53 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 24, 10.10.43.132, executor 0, partition 1, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:53 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 18) in 4790 ms on 10.10.43.132 (executor 0) (1/8)
17/12/20 16:24:53 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 25, 10.10.43.132, executor 0, partition 3, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:53 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 20) in 4853 ms on 10.10.43.132 (executor 0) (2/8)
17/12/20 16:24:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.10.43.132:43277 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:53 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 26, 10.10.43.132, executor 0, partition 5, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:53 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 16) in 5205 ms on 10.10.43.132 (executor 0) (3/8)
17/12/20 16:24:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.10.43.132:33294
17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 6143280
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 6143280
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 6143280
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 6143280
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 6143280
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 6143280
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 6143280
uncompressedSizes(7): 0
totalSize: 6143280


17/12/20 16:24:53 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 6143280
totalSize: 6143280


17/12/20 16:24:53 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 195 bytes
17/12/20 16:24:53 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 27, 10.10.43.132, executor 0, partition 7, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:53 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 19) in 4958 ms on 10.10.43.132 (executor 0) (4/8)
17/12/20 16:24:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 28, 10.10.43.131, executor 1, partition 0, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:54 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 17) in 5439 ms on 10.10.43.131 (executor 1) (5/8)
17/12/20 16:24:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.10.43.131:37275 (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:24:54 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 29, 10.10.43.131, executor 1, partition 2, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:54 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 22) in 5433 ms on 10.10.43.131 (executor 1) (6/8)
17/12/20 16:24:54 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.10.43.131:52928
17/12/20 16:24:54 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@3ec568be


17/12/20 16:24:54 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 30, 10.10.43.131, executor 1, partition 4, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:54 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 23) in 5494 ms on 10.10.43.131 (executor 1) (7/8)
17/12/20 16:24:54 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 31, 10.10.43.131, executor 1, partition 6, NODE_LOCAL, 4614 bytes)
17/12/20 16:24:54 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 21) in 5548 ms on 10.10.43.131 (executor 1) (8/8)
17/12/20 16:24:54 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
17/12/20 16:24:54 INFO DAGScheduler: ShuffleMapStage 2 (mapPartitions at VertexRDD.scala:356) finished in 7.639 s
17/12/20 16:24:54 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:24:54 INFO DAGScheduler: running: Set(ShuffleMapStage 3)
17/12/20 16:24:54 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4, ResultStage 8)
17/12/20 16:24:54 INFO DAGScheduler: failed: Set()
17/12/20 16:24:58 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 28) in 4402 ms on 10.10.43.131 (executor 1) (1/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 29) in 4106 ms on 10.10.43.131 (executor 1) (2/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 31) in 4103 ms on 10.10.43.131 (executor 1) (3/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 30) in 4117 ms on 10.10.43.131 (executor 1) (4/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 24) in 5212 ms on 10.10.43.132 (executor 0) (5/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 25) in 5177 ms on 10.10.43.132 (executor 0) (6/8)
17/12/20 16:24:58 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 27) in 5082 ms on 10.10.43.132 (executor 0) (7/8)
17/12/20 16:24:59 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 26) in 5539 ms on 10.10.43.132 (executor 0) (8/8)
17/12/20 16:24:59 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
17/12/20 16:24:59 INFO DAGScheduler: ShuffleMapStage 3 (map at GraphxNWeight.scala:87) finished in 10.214 s
17/12/20 16:24:59 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:24:59 INFO DAGScheduler: running: Set()
17/12/20 16:24:59 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ShuffleMapStage 4, ResultStage 8)
17/12/20 16:24:59 INFO DAGScheduler: failed: Set()
17/12/20 16:24:59 INFO DAGScheduler: Submitting ShuffleMapStage 4 (ReplicatedVertexView.upgrade(true, true) - shippedVerts true true (broadcast) MapPartitionsRDD[20] at mapPartitions at VertexRDDImpl.scala:247), which has no missing parents
17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:24:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.1 KB, free 366.0 MB)
17/12/20 16:24:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)
17/12/20 16:24:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.10.43.130:44564 (size: 2.8 KB, free: 366.3 MB)
17/12/20 16:24:59 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1006
17/12/20 16:24:59 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (ReplicatedVertexView.upgrade(true, true) - shippedVerts true true (broadcast) MapPartitionsRDD[20] at mapPartitions at VertexRDDImpl.scala:247) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:24:59 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks
17/12/20 16:24:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 32, 10.10.43.131, executor 1, partition 0, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 33, 10.10.43.132, executor 0, partition 1, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 34, 10.10.43.131, executor 1, partition 2, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 35, 10.10.43.132, executor 0, partition 3, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 36, 10.10.43.131, executor 1, partition 4, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 37, 10.10.43.132, executor 0, partition 5, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 38, 10.10.43.131, executor 1, partition 6, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 39, 10.10.43.132, executor 0, partition 7, NODE_LOCAL, 4934 bytes)
17/12/20 16:24:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.10.43.131:37275 (size: 2.8 KB, free: 366.3 MB)
17/12/20 16:24:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.10.43.131:52928
17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 37571746
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 37571746
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 37571746
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 37571746
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 37571746
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 37571746
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 37571746
uncompressedSizes(7): 0
totalSize: 37571746


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 37571746
totalSize: 37571746


17/12/20 16:24:59 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 195 bytes
17/12/20 16:24:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.10.43.131:52928
17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 1957438
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 1957438
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 1957438
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 1957438
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 1957438
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 1957438
uncompressedSizes(6): 320056
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 1957438
uncompressedSizes(7): 320056
totalSize: 4197830


17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 320056
uncompressedSizes(1): 320056
uncompressedSizes(2): 320056
uncompressedSizes(3): 320056
uncompressedSizes(4): 320056
uncompressedSizes(5): 320056
uncompressedSizes(6): 320056
uncompressedSizes(7): 1957438
totalSize: 4197830


17/12/20 16:24:59 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 195 bytes
17/12/20 16:24:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.10.43.132:43277 (size: 2.8 KB, free: 366.3 MB)
17/12/20 16:24:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.10.43.132:33294
17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@74ac876f


17/12/20 16:24:59 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.10.43.132:33294
17/12/20 16:24:59 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@57bf2433


17/12/20 16:25:01 INFO BlockManagerInfo: Added rdd_15_2 in memory on 10.10.43.131:37275 (size: 42.2 MB, free: 324.1 MB)
17/12/20 16:25:02 INFO BlockManagerInfo: Added rdd_15_6 in memory on 10.10.43.131:37275 (size: 44.8 MB, free: 279.3 MB)
17/12/20 16:25:03 INFO BlockManagerInfo: Added rdd_15_5 in memory on 10.10.43.132:43277 (size: 43.3 MB, free: 322.9 MB)
17/12/20 16:25:03 INFO BlockManagerInfo: Added rdd_15_1 in memory on 10.10.43.132:43277 (size: 41.8 MB, free: 281.2 MB)
17/12/20 16:25:03 INFO BlockManagerInfo: Added rdd_15_0 in memory on 10.10.43.131:37275 (size: 43.7 MB, free: 235.6 MB)
17/12/20 16:25:03 INFO BlockManagerInfo: Added rdd_15_7 in memory on 10.10.43.132:43277 (size: 44.1 MB, free: 237.1 MB)
17/12/20 16:25:03 INFO BlockManagerInfo: Added rdd_15_3 in memory on 10.10.43.132:43277 (size: 38.0 MB, free: 199.1 MB)
17/12/20 16:25:04 INFO BlockManagerInfo: Added rdd_15_4 in memory on 10.10.43.131:37275 (size: 36.4 MB, free: 199.2 MB)
17/12/20 16:25:06 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 34) in 7092 ms on 10.10.43.131 (executor 1) (1/8)
17/12/20 16:25:06 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 38) in 7369 ms on 10.10.43.131 (executor 1) (2/8)
17/12/20 16:25:07 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 32) in 7741 ms on 10.10.43.131 (executor 1) (3/8)
17/12/20 16:25:07 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 36) in 7996 ms on 10.10.43.131 (executor 1) (4/8)
17/12/20 16:25:07 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 35) in 8408 ms on 10.10.43.132 (executor 0) (5/8)
17/12/20 16:25:07 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 33) in 8608 ms on 10.10.43.132 (executor 0) (6/8)
17/12/20 16:25:07 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 39) in 8634 ms on 10.10.43.132 (executor 0) (7/8)
17/12/20 16:25:08 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 37) in 8755 ms on 10.10.43.132 (executor 0) (8/8)
17/12/20 16:25:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
17/12/20 16:25:08 INFO DAGScheduler: ShuffleMapStage 4 (mapPartitions at VertexRDDImpl.scala:247) finished in 8.760 s
17/12/20 16:25:08 INFO DAGScheduler: looking for newly runnable stages
17/12/20 16:25:08 INFO DAGScheduler: running: Set()
17/12/20 16:25:08 INFO DAGScheduler: waiting: Set(ShuffleMapStage 5, ShuffleMapStage 6, ShuffleMapStage 7, ResultStage 8)
17/12/20 16:25:08 INFO DAGScheduler: failed: Set()
17/12/20 16:25:08 INFO DAGScheduler: Submitting ShuffleMapStage 5 (GraphImpl.aggregateMessages - preAgg MapPartitionsRDD[24] at mapPartitions at GraphImpl.scala:207), which has no missing parents
17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
codecName: org.apache.spark.io.LZFCompressionCodec

17/12/20 16:25:08 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 5.7 KB, free 366.0 MB)
17/12/20 16:25:08 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.0 KB, free 366.0 MB)
17/12/20 16:25:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.10.43.130:44564 (size: 3.0 KB, free: 366.3 MB)
17/12/20 16:25:08 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/12/20 16:25:08 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 5 (GraphImpl.aggregateMessages - preAgg MapPartitionsRDD[24] at mapPartitions at GraphImpl.scala:207) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
17/12/20 16:25:08 INFO TaskSchedulerImpl: Adding task set 5.0 with 8 tasks
17/12/20 16:25:08 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 40, 10.10.43.131, executor 1, partition 0, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 41, 10.10.43.132, executor 0, partition 1, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 42, 10.10.43.131, executor 1, partition 2, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 43, 10.10.43.132, executor 0, partition 3, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 44, 10.10.43.131, executor 1, partition 4, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 45, 10.10.43.132, executor 0, partition 5, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 46, 10.10.43.131, executor 1, partition 6, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 47, 10.10.43.132, executor 0, partition 7, NODE_LOCAL, 4934 bytes)
17/12/20 16:25:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.10.43.131:37275 (size: 3.0 KB, free: 199.2 MB)
17/12/20 16:25:08 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.10.43.132:43277 (size: 3.0 KB, free: 199.1 MB)
17/12/20 16:25:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52928
17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 15934088
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 15934088
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 15934088
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 15934088
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 15934088
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 15934088
uncompressedSizes(6): 0
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 15934088
uncompressedSizes(7): 0
totalSize: 15934088


17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 0
uncompressedSizes(1): 0
uncompressedSizes(2): 0
uncompressedSizes(3): 0
uncompressedSizes(4): 0
uncompressedSizes(5): 0
uncompressedSizes(6): 0
uncompressedSizes(7): 15934088
totalSize: 15934088


17/12/20 16:25:08 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 7 is 195 bytes
17/12/20 16:25:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33294
17/12/20 16:25:08 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@daa6b33


17/12/20 16:25:10 INFO BlockManagerInfo: Added rdd_18_2 in memory on 10.10.43.131:37275 (size: 25.3 MB, free: 173.9 MB)
17/12/20 16:25:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.10.43.131:52928
17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(0): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 11971516
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(1): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 11971516
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(2): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 11971516
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(3): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 11971516
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(4): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 11971516
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(5): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 11971516
uncompressedSizes(6): 2368500
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(6): KMGetBlockInfo:
location: BlockManagerId(1, 10.10.43.131, 37275, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 11971516
uncompressedSizes(7): 2368500
totalSize: 28551016


17/12/20 16:25:10 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses(7): KMGetBlockInfo:
location: BlockManagerId(0, 10.10.43.132, 43277, None)
uncompressedSizes(0): 2368500
uncompressedSizes(1): 2368500
uncompressedSizes(2): 2368500
uncompressedSizes(3): 2368500
uncompressedSizes(4): 2368500
uncompressedSizes(5): 2368500
uncompressedSizes(6): 2368500
uncompressedSizes(7): 11971516
totalSize: 28551016


17/12/20 16:25:10 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 195 bytes
17/12/20 16:25:10 INFO BlockManagerInfo: Added rdd_18_4 in memory on 10.10.43.131:37275 (size: 25.3 MB, free: 148.6 MB)
17/12/20 16:25:10 INFO BlockManagerInfo: Added rdd_18_0 in memory on 10.10.43.131:37275 (size: 25.3 MB, free: 123.3 MB)
17/12/20 16:25:10 INFO BlockManagerInfo: Added rdd_18_6 in memory on 10.10.43.131:37275 (size: 25.3 MB, free: 98.0 MB)
17/12/20 16:25:11 INFO BlockManagerInfo: Added rdd_18_7 in memory on 10.10.43.132:43277 (size: 25.3 MB, free: 173.8 MB)
17/12/20 16:25:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.10.43.132:33294
17/12/20 16:25:11 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: [B@3377e393


17/12/20 16:25:11 INFO BlockManagerInfo: Added rdd_18_5 in memory on 10.10.43.132:43277 (size: 25.3 MB, free: 148.5 MB)
17/12/20 16:25:11 INFO BlockManagerInfo: Added rdd_18_3 in memory on 10.10.43.132:43277 (size: 25.3 MB, free: 123.2 MB)
17/12/20 16:25:12 INFO BlockManagerInfo: Added rdd_18_1 in memory on 10.10.43.132:43277 (size: 25.3 MB, free: 98.0 MB)
17/12/20 16:26:32 ERROR TaskSchedulerImpl: Lost executor 0 on 10.10.43.132: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:26:32 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 41, 10.10.43.132, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:26:32 WARN TaskSetManager: Lost task 7.0 in stage 5.0 (TID 47, 10.10.43.132, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:26:32 WARN TaskSetManager: Lost task 3.0 in stage 5.0 (TID 43, 10.10.43.132, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:26:32 WARN TaskSetManager: Lost task 5.0 in stage 5.0 (TID 45, 10.10.43.132, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:26:32 INFO DAGScheduler: Executor lost: 0 (epoch 5)
17/12/20 16:26:32 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_5 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_7 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_3 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_7 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_1 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_5 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_1 !
17/12/20 16:26:32 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_3 !
17/12/20 16:26:32 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.10.43.132, 43277, None)
17/12/20 16:26:32 INFO BlockManagerMaster: Removed 0 successfully in removeExecutor
17/12/20 16:26:32 INFO DAGScheduler: Shuffle files lost for executor: 0 (epoch 5)
17/12/20 16:26:32 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 0 (4/8, false)
17/12/20 16:26:32 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 0 (4/8, false)
17/12/20 16:26:32 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 0 (4/8, false)
17/12/20 16:26:32 INFO ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 0 (4/8, false)
17/12/20 16:26:32 INFO ShuffleMapStage: ShuffleMapStage 4 is now unavailable on executor 0 (4/8, false)
17/12/20 16:26:36 ERROR TaskSchedulerImpl: Ignoring update with state FAILED for TID 43 because its task set is gone (this is likely the result of receiving duplicate task finished status updates) or its executor has been marked as failed.
17/12/20 16:26:36 WARN CoarseGrainedSchedulerBackend$DriverEndpoint: Ignored task status update (43 state FAILED) from unknown executor with ID 0
17/12/20 16:27:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171220162431-0007/0 is now EXITED (Command exited with code 1)
17/12/20 16:27:04 INFO StandaloneSchedulerBackend: Executor app-20171220162431-0007/0 removed: Command exited with code 1
17/12/20 16:27:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171220162431-0007/2 on worker-20171220145248-10.10.43.132-42694 (10.10.43.132:42694) with 4 cores
17/12/20 16:27:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20171220162431-0007/2 on hostPort 10.10.43.132:42694 with 4 cores, 1024.0 MB RAM
17/12/20 16:27:04 INFO BlockManagerMaster: Removal of executor 0 requested
17/12/20 16:27:04 INFO BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster.
17/12/20 16:27:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 0
17/12/20 16:27:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171220162431-0007/2 is now RUNNING
17/12/20 16:27:06 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.43.132:33318) with ID 2
17/12/20 16:27:06 INFO TaskSetManager: Starting task 5.1 in stage 5.0 (TID 48, 10.10.43.132, executor 2, partition 5, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:06 INFO TaskSetManager: Starting task 3.1 in stage 5.0 (TID 49, 10.10.43.132, executor 2, partition 3, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:06 INFO TaskSetManager: Starting task 7.1 in stage 5.0 (TID 50, 10.10.43.132, executor 2, partition 7, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:06 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 51, 10.10.43.132, executor 2, partition 1, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:07 INFO BlockManagerMasterEndpoint: Registering block manager 10.10.43.132:42175 with 366.3 MB RAM, BlockManagerId(2, 10.10.43.132, 42175, None)
17/12/20 16:27:31 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 127915 ms exceeds timeout 120000 ms
17/12/20 16:27:31 ERROR TaskSchedulerImpl: Lost executor 1 on 10.10.43.131: Executor heartbeat timed out after 127915 ms
17/12/20 16:27:31 WARN TaskSetManager: Lost task 4.0 in stage 5.0 (TID 44, 10.10.43.131, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 127915 ms
17/12/20 16:27:31 WARN TaskSetManager: Lost task 6.0 in stage 5.0 (TID 46, 10.10.43.131, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 127915 ms
17/12/20 16:27:31 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 40, 10.10.43.131, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 127915 ms
17/12/20 16:27:31 WARN TaskSetManager: Lost task 2.0 in stage 5.0 (TID 42, 10.10.43.131, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 127915 ms
17/12/20 16:27:31 INFO DAGScheduler: Executor lost: 1 (epoch 13)
17/12/20 16:27:31 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_6 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_4 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_6 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_0 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_2 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_4 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_18_0 !
17/12/20 16:27:31 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_15_2 !
17/12/20 16:27:31 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 10.10.43.131, 37275, None)
17/12/20 16:27:31 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
17/12/20 16:27:31 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 13)
17/12/20 16:27:31 INFO ShuffleMapStage: ShuffleMapStage 1 is now unavailable on executor 1 (0/8, false)
17/12/20 16:27:31 INFO ShuffleMapStage: ShuffleMapStage 0 is now unavailable on executor 1 (0/8, false)
17/12/20 16:27:31 INFO ShuffleMapStage: ShuffleMapStage 3 is now unavailable on executor 1 (0/8, false)
17/12/20 16:27:31 INFO StandaloneSchedulerBackend: Requesting to kill executor(s) 1
17/12/20 16:27:31 INFO ShuffleMapStage: ShuffleMapStage 2 is now unavailable on executor 1 (0/8, false)
17/12/20 16:27:31 INFO ShuffleMapStage: ShuffleMapStage 4 is now unavailable on executor 1 (0/8, false)
17/12/20 16:27:31 INFO StandaloneSchedulerBackend: Actual list of executor(s) to be killed is 1
17/12/20 16:27:31 INFO DAGScheduler: Host added was in lost list earlier: 10.10.43.131
17/12/20 16:27:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171220162431-0007/3 on worker-20171220145325-10.10.43.131-46830 (10.10.43.131:46830) with 4 cores
17/12/20 16:27:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20171220162431-0007/3 on hostPort 10.10.43.131:46830 with 4 cores, 1024.0 MB RAM
17/12/20 16:27:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171220162431-0007/3 is now RUNNING
17/12/20 16:27:35 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.10.43.131:52950) with ID 3
17/12/20 16:27:35 INFO TaskSetManager: Starting task 2.1 in stage 5.0 (TID 52, 10.10.43.131, executor 3, partition 2, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:35 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 53, 10.10.43.131, executor 3, partition 0, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:35 INFO TaskSetManager: Starting task 6.1 in stage 5.0 (TID 54, 10.10.43.131, executor 3, partition 6, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:35 INFO TaskSetManager: Starting task 4.1 in stage 5.0 (TID 55, 10.10.43.131, executor 3, partition 4, NODE_LOCAL, 4934 bytes)
17/12/20 16:27:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.10.43.131:39643 with 366.3 MB RAM, BlockManagerId(3, 10.10.43.131, 39643, None)
17/12/20 16:27:37 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.10.43.130:44564 in memory (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:27:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.10.43.130:44564 in memory (size: 2.6 KB, free: 366.3 MB)
17/12/20 16:27:37 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.10.43.130:44564 in memory (size: 2.8 KB, free: 366.3 MB)
17/12/20 16:27:37 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.10.43.130:44564 in memory (size: 2.3 KB, free: 366.3 MB)
17/12/20 16:27:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.10.43.131:39643 (size: 3.0 KB, free: 366.3 MB)
17/12/20 16:27:37 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.10.43.130:44564 in memory (size: 2.4 KB, free: 366.3 MB)
17/12/20 16:27:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:27:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:27:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:27:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:27:41 ERROR TaskSchedulerImpl: Lost executor 1 on 10.10.43.131: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
17/12/20 16:27:41 INFO DAGScheduler: Executor lost: 1 (epoch 21)
17/12/20 16:27:41 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
17/12/20 16:27:41 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
17/12/20 16:27:41 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 21)
17/12/20 16:27:56 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.10.43.132:42175 (size: 3.0 KB, free: 366.3 MB)
17/12/20 16:27:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33318
17/12/20 16:27:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:27:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:27:56 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:29:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:29:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:29:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:29:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:29:38 WARN TaskSetManager: Lost task 2.1 in stage 5.0 (TID 52, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:29:38 INFO TaskSetManager: Starting task 2.2 in stage 5.0 (TID 56, 10.10.43.131, executor 3, partition 2, NODE_LOCAL, 4934 bytes)
17/12/20 16:29:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33318
17/12/20 16:29:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:29:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:29:56 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:29:56 WARN TaskSetManager: Lost task 7.1 in stage 5.0 (TID 50, 10.10.43.132, executor 2): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:29:56 INFO TaskSetManager: Starting task 7.2 in stage 5.0 (TID 57, 10.10.43.132, executor 2, partition 7, NODE_LOCAL, 4934 bytes)
17/12/20 16:31:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:31:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:31:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:31:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:31:38 WARN TaskSetManager: Lost task 6.1 in stage 5.0 (TID 54, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.10.43.130:43031 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	... 3 more
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.10.43.130:43031 in 120 seconds
	... 8 more

17/12/20 16:31:38 INFO TaskSetManager: Starting task 6.2 in stage 5.0 (TID 58, 10.10.43.131, executor 3, partition 6, NODE_LOCAL, 4934 bytes)
17/12/20 16:31:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33318
17/12/20 16:31:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:31:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:31:56 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:31:56 WARN TaskSetManager: Lost task 1.1 in stage 5.0 (TID 51, 10.10.43.132, executor 2): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:31:56 INFO TaskSetManager: Starting task 1.2 in stage 5.0 (TID 59, 10.10.43.132, executor 2, partition 1, NODE_LOCAL, 4934 bytes)
17/12/20 16:33:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:33:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:33:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:33:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:33:38 WARN TaskSetManager: Lost task 2.2 in stage 5.0 (TID 56, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:33:38 INFO TaskSetManager: Starting task 2.3 in stage 5.0 (TID 60, 10.10.43.131, executor 3, partition 2, NODE_LOCAL, 4934 bytes)
17/12/20 16:33:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33318
17/12/20 16:33:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:33:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:33:56 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:33:56 WARN TaskSetManager: Lost task 7.2 in stage 5.0 (TID 57, 10.10.43.132, executor 2): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from 10.10.43.130:43031 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:216)
	at scala.util.Try$.apply(Try.scala:192)
	at scala.util.Failure.recover(Try.scala:216)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:239)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	... 3 more
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from 10.10.43.130:43031 in 120 seconds
	... 8 more

17/12/20 16:33:56 INFO TaskSetManager: Starting task 7.3 in stage 5.0 (TID 61, 10.10.43.132, executor 2, partition 7, NODE_LOCAL, 4934 bytes)
17/12/20 16:35:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:35:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:35:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:35:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:35:38 WARN TaskSetManager: Lost task 6.2 in stage 5.0 (TID 58, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:35:38 INFO TaskSetManager: Starting task 6.3 in stage 5.0 (TID 62, 10.10.43.131, executor 3, partition 6, NODE_LOCAL, 4934 bytes)
17/12/20 16:35:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.132:33318
17/12/20 16:35:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:35:56 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:35:56 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:35:56 WARN TaskSetManager: Lost task 1.2 in stage 5.0 (TID 59, 10.10.43.132, executor 2): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:35:56 INFO TaskSetManager: Starting task 1.3 in stage 5.0 (TID 63, 10.10.43.132, executor 2, partition 1, NODE_LOCAL, 4934 bytes)
17/12/20 16:37:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.10.43.131:52950
17/12/20 16:37:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
retBytes: null


17/12/20 16:37:38 INFO KMScalaKit:
******************** [KMLogInfo] ********************
statuses.length: 8


17/12/20 16:37:38 ERROR MapOutputTrackerMaster:
java.lang.NullPointerException
	at org.apache.spark.MapOutputTrackerMaster$$anonfun$getSerializedMapOutputStatuses$1.apply$mcVI$sp(MapOutputTracker.scala:573)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:570)
	at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:352)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/12/20 16:37:38 WARN TaskSetManager: Lost task 2.3 in stage 5.0 (TID 60, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

17/12/20 16:37:38 ERROR TaskSetManager: Task 2 in stage 5.0 failed 4 times; aborting job
17/12/20 16:37:38 INFO TaskSchedulerImpl: Cancelling stage 5
17/12/20 16:37:38 INFO TaskSchedulerImpl: Stage 5 was cancelled
17/12/20 16:37:38 INFO DAGScheduler: ShuffleMapStage 5 (mapPartitions at GraphImpl.scala:207) failed in 750.151 s due to Job aborted due to stage failure: Task 2 in stage 5.0 failed 4 times, most recent failure: Lost task 2.3 in stage 5.0 (TID 60, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

Driver stacktrace:
17/12/20 16:37:38 INFO DAGScheduler: Job 0 failed: saveAsTextFile at GraphxNWeight.scala:112, took 780.355227 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 5.0 failed 4 times, most recent failure: Lost task 2.3 in stage 5.0 (TID 60, 10.10.43.131, executor 3): org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2025)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2046)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2078)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1151)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)
	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)
	at com.intel.hibench.sparkbench.graph.nweight.GraphxNWeight$.nweight(GraphxNWeight.scala:112)
	at com.intel.hibench.sparkbench.graph.nweight.NWeight$.main(Driver.scala:87)
	at com.intel.hibench.sparkbench.graph.nweight.NWeight.main(Driver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:107)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:205)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:145)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1039)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:970)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1030)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:761)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:76)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:103)
	... 36 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 39 more
17/12/20 16:37:38 INFO SparkContext: Invoking stop() from shutdown hook
17/12/20 16:37:38 INFO SparkUI: Stopped Spark web UI at http://10.10.43.130:4040
17/12/20 16:37:38 INFO StandaloneSchedulerBackend: Shutting down all executors
17/12/20 16:37:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
17/12/20 16:37:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/12/20 16:37:38 INFO MemoryStore: MemoryStore cleared
17/12/20 16:37:38 INFO BlockManager: BlockManager stopped
17/12/20 16:37:38 INFO BlockManagerMaster: BlockManagerMaster stopped
17/12/20 16:37:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/12/20 16:37:38 INFO SparkContext: Successfully stopped SparkContext
17/12/20 16:37:38 INFO ShutdownHookManager: Shutdown hook called
17/12/20 16:37:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-ab484ef8-2852-422b-89c9-4310512753ac
